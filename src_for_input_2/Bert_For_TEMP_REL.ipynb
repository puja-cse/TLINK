{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 ########## Train Loss: 1.3752021789550781\n",
      "Val loss: 0.1111111111111111 ----- Val Micro F1: 0.1111111111111111 Val Macro F1: 0.049999999999999996\n",
      "\n",
      "\n",
      "EPOCH: 1 ########## Train Loss: 1.359562873840332\n",
      "Val loss: 0.1111111111111111 ----- Val Micro F1: 0.1111111111111111 Val Macro F1: 0.049999999999999996\n",
      "\n",
      "\n",
      "Model configuration: MLP with SGD Optimizer, model name: temp_model_1, loss file: temp_en_version1\n",
      "\n",
      "LR: 0.01, Epochs: 2 Batch: 8\n",
      "\n",
      "We are using 2 consequtive sentences per event where possible\n",
      "concatenating the cls token as well\n",
      "\n",
      "\n",
      "Accuracy: 0.1111111111111111 micro-F1: 0.1111111111111111 macro-F1: 0.049999999999999996\n",
      "\n",
      "----Confusion Matrix-----\n",
      "[[0 6 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 1 0 0]]\n",
      "Output (losses) saved as file: ../output_files/temp_en_version1.csv\n",
      "model saved as file: ../saved_models/temp_model_1\n",
      "Class weights: tensor([0.4906, 0.7750, 7.6154, 1.8526], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "! python train_model.py --train_path=matres/relations/sample_train.txt --epochs=2 --lr=0.01 --batch=8 --output_file=temp_en_version1 --save_model=temp_model_1 --test_path=matres/relations/sample_test.txt --dev_path=matres/relations/sample_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0 ########## Train Loss: 1.3712854385375977\n",
      "Val loss: 0.1111111111111111 ----- Val Micro F1: 0.1111111111111111 Val Macro F1: 0.049999999999999996\n",
      "\n",
      "\n",
      "EPOCH: 1 ########## Train Loss: 1.3656128644943237\n",
      "Val loss: 0.1111111111111111 ----- Val Micro F1: 0.1111111111111111 Val Macro F1: 0.049999999999999996\n",
      "\n",
      "\n",
      "Model configuration: MLP with SGD Optimizer, model name: temp_model_1, loss file: temp_en_version1\n",
      "\n",
      "LR: 0.01, Epochs: 2 Batch: 8\n",
      "\n",
      "We are using 2 consequtive sentences per event where possible\n",
      "concatenating the cls token as well\n",
      "\n",
      "\n",
      "Accuracy: 0.0 micro-F1: 0.0 macro-F1: 0.0\n",
      "\n",
      "----Confusion Matrix-----\n",
      "[[0 3 0 0]\n",
      " [0 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]]\n",
      "Output (losses) saved as file: ../output_files/temp_en_version1.csv\n",
      "model saved as file: ../saved_models/temp_model_1\n",
      "Class weights: tensor([0.4906, 0.7750, 7.6154, 1.8526], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "! python train_model.py --train_path=matres/relations_2_sentences/train_sample.txt --epochs=2 --lr=0.01 --batch=8 --output_file=temp_en_version1 --save_model=temp_model_1 --test_path=matres/relations_2_sentences/test_sample.txt --dev_path=matres/relations/sample_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! python train_model.py --train_path=matres/relations/train_timebank.txt --epochs=10 --lr=0.1 --output_file=version1 --save_model=model_1 --test_path=matres/relations/test_platinum.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 ########## Train Loss: 1.3650566339492798, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 1 ########## Train Loss: 1.367709755897522, Train Acc: 0.5\n",
      "\n",
      "Epoch: 2 ########## Train Loss: 1.3460633754730225, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 3 ########## Train Loss: 1.3607317209243774, Train Acc: 0.40625\n",
      "\n",
      "Epoch: 4 ########## Train Loss: 1.3442292213439941, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 5 ########## Train Loss: 1.3294134140014648, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 6 ########## Train Loss: 1.343810796737671, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 7 ########## Train Loss: 1.352225422859192, Train Acc: 0.40625\n",
      "\n",
      "Epoch: 8 ########## Train Loss: 1.2964842319488525, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 9 ########## Train Loss: 1.3133196830749512, Train Acc: 0.5\n",
      "\n",
      "Epoch: 10 ########## Train Loss: 1.2863720655441284, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 11 ########## Train Loss: 1.3013889789581299, Train Acc: 0.5\n",
      "\n",
      "Epoch: 12 ########## Train Loss: 1.28575599193573, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 13 ########## Train Loss: 1.2522449493408203, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 14 ########## Train Loss: 1.277643084526062, Train Acc: 0.5\n",
      "\n",
      "Epoch: 15 ########## Train Loss: 1.2670528888702393, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 16 ########## Train Loss: 1.2923715114593506, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 17 ########## Train Loss: 1.1822432279586792, Train Acc: 0.65625\n",
      "\n",
      "Epoch: 18 ########## Train Loss: 1.262916922569275, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 19 ########## Train Loss: 1.2484022378921509, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 20 ########## Train Loss: 1.2537380456924438, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 21 ########## Train Loss: 1.2718397378921509, Train Acc: 0.5\n",
      "\n",
      "Epoch: 22 ########## Train Loss: 1.2755393981933594, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 23 ########## Train Loss: 1.276349425315857, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 24 ########## Train Loss: 1.2171965837478638, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 25 ########## Train Loss: 1.3065752983093262, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 26 ########## Train Loss: 1.2147184610366821, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 27 ########## Train Loss: 1.1982167959213257, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 28 ########## Train Loss: 1.3432483673095703, Train Acc: 0.375\n",
      "\n",
      "Epoch: 29 ########## Train Loss: 1.1746370792388916, Train Acc: 0.65625\n",
      "\n",
      "Epoch: 30 ########## Train Loss: 1.146653175354004, Train Acc: 0.625\n",
      "\n",
      "Epoch: 31 ########## Train Loss: 1.2035034894943237, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 32 ########## Train Loss: 1.3443540334701538, Train Acc: 0.375\n",
      "\n",
      "Epoch: 33 ########## Train Loss: 1.3141249418258667, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 34 ########## Train Loss: 1.226917028427124, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 35 ########## Train Loss: 1.2024188041687012, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 36 ########## Train Loss: 1.3388519287109375, Train Acc: 0.375\n",
      "\n",
      "Epoch: 37 ########## Train Loss: 1.245619773864746, Train Acc: 0.5\n",
      "\n",
      "Epoch: 38 ########## Train Loss: 1.2858686447143555, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 39 ########## Train Loss: 1.1834259033203125, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 40 ########## Train Loss: 1.4175102710723877, Train Acc: 0.3125\n",
      "\n",
      "Epoch: 41 ########## Train Loss: 1.1626306772232056, Train Acc: 0.625\n",
      "\n",
      "Epoch: 42 ########## Train Loss: 1.2741907835006714, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 43 ########## Train Loss: 1.1739252805709839, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 44 ########## Train Loss: 1.229357123374939, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 45 ########## Train Loss: 1.2320075035095215, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 46 ########## Train Loss: 1.3410766124725342, Train Acc: 0.40625\n",
      "\n",
      "Epoch: 47 ########## Train Loss: 1.2503503561019897, Train Acc: 0.5\n",
      "\n",
      "Epoch: 48 ########## Train Loss: 1.355785846710205, Train Acc: 0.40625\n",
      "\n",
      "Epoch: 49 ########## Train Loss: 1.2219691276550293, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 50 ########## Train Loss: 1.1749666929244995, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 51 ########## Train Loss: 1.245200514793396, Train Acc: 0.5\n",
      "\n",
      "Epoch: 52 ########## Train Loss: 1.2977136373519897, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 53 ########## Train Loss: 1.424522876739502, Train Acc: 0.28125\n",
      "\n",
      "Epoch: 54 ########## Train Loss: 1.2153831720352173, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 55 ########## Train Loss: 1.1369835138320923, Train Acc: 0.625\n",
      "\n",
      "Epoch: 56 ########## Train Loss: 1.183998942375183, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 57 ########## Train Loss: 1.355186104774475, Train Acc: 0.375\n",
      "\n",
      "Epoch: 58 ########## Train Loss: 1.2139326333999634, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 59 ########## Train Loss: 1.23050856590271, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 60 ########## Train Loss: 1.1695352792739868, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 61 ########## Train Loss: 1.2730708122253418, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 62 ########## Train Loss: 1.2013723850250244, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 63 ########## Train Loss: 1.2831617593765259, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 64 ########## Train Loss: 1.2318246364593506, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 65 ########## Train Loss: 1.2507891654968262, Train Acc: 0.5\n",
      "\n",
      "Epoch: 66 ########## Train Loss: 1.2834380865097046, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 67 ########## Train Loss: 1.244968056678772, Train Acc: 0.5\n",
      "\n",
      "Epoch: 68 ########## Train Loss: 1.3426624536514282, Train Acc: 0.40625\n",
      "\n",
      "Epoch: 69 ########## Train Loss: 1.242950439453125, Train Acc: 0.5\n",
      "\n",
      "Epoch: 70 ########## Train Loss: 1.2160868644714355, Train Acc: 0.53125\n",
      "\n",
      "Epoch: 71 ########## Train Loss: 1.3080034255981445, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 72 ########## Train Loss: 1.260473608970642, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 73 ########## Train Loss: 1.1657170057296753, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 74 ########## Train Loss: 1.3259719610214233, Train Acc: 0.40625\n",
      "\n",
      "Epoch: 75 ########## Train Loss: 1.2057591676712036, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 76 ########## Train Loss: 1.2494984865188599, Train Acc: 0.5\n",
      "\n",
      "Epoch: 77 ########## Train Loss: 1.150961995124817, Train Acc: 0.625\n",
      "\n",
      "Epoch: 78 ########## Train Loss: 1.2744137048721313, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 79 ########## Train Loss: 1.275494933128357, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 80 ########## Train Loss: 1.2497012615203857, Train Acc: 0.5\n",
      "\n",
      "Epoch: 81 ########## Train Loss: 1.2728490829467773, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 82 ########## Train Loss: 1.1587904691696167, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 83 ########## Train Loss: 1.1972683668136597, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 84 ########## Train Loss: 1.302114486694336, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 85 ########## Train Loss: 1.1959724426269531, Train Acc: 0.5625\n",
      "\n",
      "Epoch: 86 ########## Train Loss: 1.2542790174484253, Train Acc: 0.5\n",
      "\n",
      "Epoch: 87 ########## Train Loss: 1.2478212118148804, Train Acc: 0.5\n",
      "\n",
      "Epoch: 88 ########## Train Loss: 1.2407782077789307, Train Acc: 0.5\n",
      "\n",
      "Epoch: 89 ########## Train Loss: 1.3417389392852783, Train Acc: 0.40625\n",
      "\n",
      "Epoch: 90 ########## Train Loss: 1.2704017162322998, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 91 ########## Train Loss: 1.280016303062439, Train Acc: 0.46875\n",
      "\n",
      "Epoch: 92 ########## Train Loss: 1.1529061794281006, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 93 ########## Train Loss: 1.2992982864379883, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 94 ########## Train Loss: 1.307967185974121, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 95 ########## Train Loss: 1.24191415309906, Train Acc: 0.5\n",
      "\n",
      "Epoch: 96 ########## Train Loss: 1.304807424545288, Train Acc: 0.4375\n",
      "\n",
      "Epoch: 97 ########## Train Loss: 1.1557883024215698, Train Acc: 0.59375\n",
      "\n",
      "Epoch: 98 ########## Train Loss: 1.249275803565979, Train Acc: 0.5\n",
      "\n",
      "Epoch: 99 ########## Train Loss: 1.340542197227478, Train Acc: 0.40625\n",
      "\n",
      "<class 'list'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "! python train_model.py --train_path=EN/train_v2.txt --epochs=100 --lr=0.01 --output_file=en_version2 --save_model=en_model_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaModel, XLMRobertaTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", return_tensors='pt')\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\", output_hidden_states=False, output_attentions=False)\n",
    "#tokenizer = XLMRobertaModel(\"xlm-roberta\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= \"I am Loving it 's --- test U.S. jet .\"\n",
    "encodings = tokenizer(X, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "outputs = model (**encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    87,   444, 39701,   214,   442, 28229,  3034,     2]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁I',\n",
       " '▁am',\n",
       " '▁Lov',\n",
       " 'ing',\n",
       " '▁it',\n",
       " \"▁'\",\n",
       " 's',\n",
       " '▁---',\n",
       " '▁test',\n",
       " '▁U',\n",
       " '.',\n",
       " 'S',\n",
       " '.',\n",
       " '▁je',\n",
       " 't',\n",
       " '▁',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(encodings['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "from customDataHandler import customDataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "from BertTempRel import BertTempRel\n",
    "path = 'D:/ML_Projects/RUG_Research_Training/event_anchorability/dataset/EN/samples_3.txt'\n",
    "train_dataset= customDataset(path=path)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "#model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", return_tensors='pt')\n",
    "\n",
    "model = BertTempRel()\n",
    "model = model.to(device)\n",
    "#optimizier = torch.optim.SGD(params=model.parameters(), lr=0.1)\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    loc1 = batch['location1']\n",
    "    loc2 = batch['location2']\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask= attention_mask.to(device)\n",
    "    loc1 = loc1.to(device)\n",
    "    loc2 = loc2.to(device)\n",
    "    #print(type(input_ids))\n",
    "    #outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, event1 = loc1, event2 =loc2 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2565, 0.2346, 0.2531, 0.2557],\n",
       "        [0.2560, 0.2382, 0.2553, 0.2505],\n",
       "        [0.2585, 0.2321, 0.2561, 0.2533]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()\n",
    "optimizier = torch.optim.SGD(params=list(model.parameters()), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x0000021B1A72FBC0>\n"
     ]
    }
   ],
   "source": [
    "from BertTempRel import BertTempRel\n",
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "model = BertTempRel()\n",
    "#model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"one.. . two \", return_tensors=\"pt\")\n",
    "model.eval()\n",
    "outputs = model(inputs['input_ids'], inputs['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 1632,    5,    5,    6,    5, 6626,    2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one..', '.', 'two']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"one.. . two\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mdeberta-v3-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/mdeberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'deberta.embeddings.word_embeddings._weight', 'mask_predictions.classifier.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DebertaV2Model\n",
    "import torch\n",
    "model_name = \"microsoft/mdeberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,  use_fast=False)\n",
    "model = DebertaV2Model.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"Hello, my dog is doing cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '▁Hello', ',', '▁my', '▁dog', '▁is', '▁do', 'ing', '▁cute', '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TENSOR operations if they are working fine or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "locs1 = [1, 4, 2]\n",
    "locs2 = [5, 7, 4]\n",
    "t1 = torch.Tensor([[ [2,3,1,5], [11,12,13,14], [2,3,1,5], [11,12,13,14], [1,6,4,5],[101,102, 103,104], [1,6,4,5],[101,102, 103,104] ],\n",
    "                   [ [1,6,4,5],[101,102, 103,104], [1,6,4,5],[101,102, 103,104], [2,3,1,5], [11,12,13,14], [2,3,1,5], [11,12,13,14], ],\n",
    "                   [ [1,9,8,0],[601,602,603,604], [1,9,8,0],[601,602,603,604], [2,3,1,5], [11,12,13,14], [2,3,1,5], [11,12,13,14],   ] ])\n",
    "t2 = torch.Tensor([ [ [10,1,11,12], [33,32,34,77], [10,1,11,12], [33,32,34,77], [17,18,19,20],[61,62,63,78], [17,18,19,20],[61,62,63,78]  ],\n",
    "                    [ [13,14,15,16], [42,43,41,78],[13,14,15,16], [42,43,41,78], [10,1,11,12], [33,32,34,77], [10,1,11,12], [33,32,34,77]  ],\n",
    "                    [ [17,18,19,20],[61,62,63,78], [17,18,19,20],[61,62,63,78], [13,14,15,16], [42,43,41,78],[13,14,15,16], [42,43,41,78]  ] ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 4])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[33., 32., 34., 77.],\n",
       "        [10.,  1., 11., 12.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2[0, 1:3 , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[  2.0000,   3.0000,   1.0000,   5.0000,  23.2500,  20.7500,  24.5000,\n",
      "          46.5000],\n",
      "        [ 51.0000,  54.0000,  53.5000,  54.5000,  17.6667,  11.3333,  18.6667,\n",
      "          33.6667],\n",
      "        [301.0000, 305.5000, 305.5000, 302.0000,  39.0000,  40.0000,  41.0000,\n",
      "          49.0000]])\n"
     ]
    }
   ],
   "source": [
    "complete_tensor= torch.empty(0,2*4)\n",
    "for i in range(len(locs1)):\n",
    "    l1= locs1[i]\n",
    "    l2 = locs2[i]\n",
    "    tok1 = torch.mean(t1[i, 0 : l1, :], dim =0)\n",
    "    tok2 = torch.mean(t2[i, l1 : l2, :], dim =0)\n",
    "    print(type(tok1))\n",
    "    print(type(tok2))\n",
    "    tok = torch.cat((tok1,tok2), dim =0 )\n",
    "    complete_tensor = torch.vstack((complete_tensor, tok))\n",
    "print(complete_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGE Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.,   3.,   1.,   5.,  10.,   1.,  11.,  12.],\n",
       "        [101., 102., 103., 104.,  42.,  43.,  41.,  78.],\n",
       "        [601., 602., 603., 604.,  61.,  62.,  63.,  78.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=[0 , 1, 1]\n",
    "t= torch.cat((t1[[i for i in range(3)], list1], t2[[i for i in range(3)], list1]), 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECKING IF THE DATALOADER IS WORKING FINE OR NOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from customDataHandler import customDataset\n",
    "path = 'D:/ML_Projects/RUG_Research_Training/event_anchorability/dataset/EN/samples_3.txt'\n",
    "train_dataset= customDataset(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<customDataHandler.customDataset at 0x213d9072c20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sentence 1:  ['People have predicted his demise so many times , and the US has tried to hasten it on several occasions . [SEP]', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions . [SEP]', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions . [SEP]']\n",
      "0 Sentence 2:  ['People have predicted his demise so many times , and the US has tried to hasten it on several occasions .', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions .', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions .']\n",
      "0 Event 1:  tensor([2, 2, 2])\n",
      "0 Event 2:  tensor([37, 26, 35])\n",
      "0 Label  tensor([3, 0, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (idx, batch) in enumerate(dataloader):\n",
    "\n",
    "    # Print the 'text' data of the batch\n",
    "    print(idx, 'Sentence 1: ', batch['sentence1'])\n",
    "    print(idx, 'Sentence 2: ', batch['sentence2'])\n",
    "    print(idx, 'Event 1: ', batch['location1']) \n",
    "    print(idx, 'Event 2: ', batch['location2'])\n",
    "    # Print the 'class' data of batch\n",
    "    print(idx, 'Label ', batch['label'], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sentence 1:  ['People have predicted his demise so many times , and the US has tried to hasten it on several occasions .', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions .', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions .']\n",
      "0 Sentence 2:  ['People have predicted his demise so many times , and the US has tried to hasten it on several occasions .', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions .', 'People have predicted his demise so many times , and the US has tried to hasten it on several occasions .']\n",
      "105\n",
      "0 Event 1:  tensor([3, 3, 3])\n",
      "0 Event 2:  tensor([109, 120, 118])\n",
      "0 Label  tensor([0, 3, 3]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, XLMRobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "for (idx, batch) in enumerate(dataloader):\n",
    "\n",
    "    # Print the 'text' data of the batch\n",
    "    print(idx, 'Sentence 1: ', batch['sentence1'])\n",
    "    print(idx, 'Sentence 2: ', batch['sentence2'])\n",
    "    batch['location1'] +=1\n",
    "    print(len(batch['sentence1'][0]))\n",
    "    for i in range(len(batch['label'])):\n",
    "        batch['location2'][i] += len(batch['sentence1'][i])\n",
    "    print(idx, 'Event 1: ', batch['location1']) \n",
    "    print(idx, 'Event 2: ', batch['location2'])\n",
    "    # Print the 'class' data of batch\n",
    "    print(idx, 'Label ', batch['label'], '\\n')\n",
    "\n",
    "\n",
    "\n",
    "inputs = tokenizer(\"one two three four five\", return_tensors=\"pt\")\n",
    "model.eval()\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 1061.036MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\BertTempRel.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return nn.functional.softmax(x)\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\train_model.py\", line 78, in <module>\n",
      "    loss, acc = train(model)\n",
      "  File \"d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src\\train_model.py\", line 51, in train\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 734.00 MiB (GPU 0; 2.00 GiB total capacity; 1.38 GiB already allocated; 0 bytes free; 1.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "! python train_model.py --epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Meta-Llama-3-8B-Instruct\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\DELL\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  25%|██▌       | 1/4 [00:10<00:30, 10.16s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 2/4 [00:27<00:28, 14.48s/it]\n",
      "Loading checkpoint shards:  75%|███████▌  | 3/4 [00:56<00:21, 21.00s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.08s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.97s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src2\\prompt_LLM.py\", line 121, in <module>\n",
      "    main()\n",
      "  File \"d:\\ML_Projects\\RUG_Research_Training\\event_anchorability\\src2\\prompt_LLM.py\", line 105, in main\n",
      "    pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\transformers\\pipelines\\__init__.py\", line 1108, in pipeline\n",
      "    return pipeline_class(model=model, framework=framework, task=task, **kwargs)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\transformers\\pipelines\\text_generation.py\", line 96, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 883, in __init__\n",
      "    self.model.to(self.device)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\transformers\\modeling_utils.py\", line 2724, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1145, in to\n",
      "    return self._apply(convert)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 797, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 2 more times]\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 820, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1143, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python prompt_LLM.py --model meta-llama/Meta-Llama-3-8B-Instruct --test_dir matres/relations_2_sentences/test_sample.txt --prompt_dir prompts/four_shot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/flan-t5-small\n",
      "EQUAL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\DELL\\anaconda3\\envs\\py3.10\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `10` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python prompt_LLM.py --test_dir matres/relations/test_platinum.txt --prompt_dir prompts/four_shot.txt --n_shot=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python prompt_LLM.py --model meta-llama/Meta-Llama-3-8B-Instruct --test_dir matres/relations_2_sentences/test_sample.txt --prompt_dir prompts/four_shot.txt --n_shot=4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
